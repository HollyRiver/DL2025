{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac18bf",
   "metadata": {},
   "source": [
    "# 13wk-2: (강화학습) – Bandit 환경 설계 및 풀이, 4x4 Grid World 게임설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20c324",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6caaef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "#---#\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e86b8",
   "metadata": {},
   "source": [
    "## 2. 주요 코드 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d37aed-8d03-49d6-a342-8f9e64c6ccdd",
   "metadata": {},
   "source": [
    "`-` 클래스로 Agent와 Environment 구현\n",
    "\n",
    "```Python\n",
    "class Bandit :\n",
    "    \"\"\"\n",
    "    Bandit Game Environment\n",
    "    \"\"\"\n",
    "    def __init__(self) :\n",
    "        self.reward = None\n",
    "\n",
    "    def step(self, action) :\n",
    "        \"\"\"\n",
    "        보통 state, reward, terminated를 출력하나, 여기선 terminated밖에 없음\n",
    "        \"\"\"\n",
    "        if action == 0 :\n",
    "            self.reward = 1\n",
    "        else :\n",
    "            self.reward = 10\n",
    "\n",
    "        return self.reward\n",
    "\n",
    "\n",
    "class Agent :\n",
    "    \"\"\"\n",
    "    Bandit Game Agent\n",
    "    \"\"\"\n",
    "    def __init__(self) :\n",
    "        self.n_experience = 0\n",
    "\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.actions = collections.deque(maxlen = 500)\n",
    "        self.rewards = collections.deque(maxlen = 500)\n",
    "\n",
    "        self.action_space = [0, 1]\n",
    "        self.q_table = None\n",
    "\n",
    "    def act(self) :\n",
    "        \"\"\"\n",
    "        매우 낮은 확률로 두 그룹이 전부 관찰되지 않을 수 있음(2/100조 정도이긴 함)\n",
    "        \"\"\"\n",
    "        if self.n_experience < 20 :\n",
    "            self.action = np.random.choice(self.action_space)\n",
    "        else :\n",
    "            self.action = self.q_table.argmax()\n",
    "\n",
    "    def save_experience(self) :\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.n_experience += 1\n",
    "\n",
    "    def learn(self) :\n",
    "        actions = np.array(self.actions)\n",
    "        rewards = np.array(self.rewards)\n",
    "\n",
    "        q0 = rewards[actions == 0].mean()\n",
    "        q1 = rewards[actions == 1].mean()\n",
    "\n",
    "        self.q_table = np.array([q0, q1])\n",
    "```\n",
    "\n",
    "`-` 학습\n",
    "\n",
    "```Python\n",
    "env = Bandit()\n",
    "player = Agent()\n",
    "\n",
    "for _ in range(100) :\n",
    "    ## step1 : agent action\n",
    "    player.act()\n",
    "\n",
    "    ## step2 : action -> state, reward\n",
    "    player.reward = env.step(player.action)\n",
    "\n",
    "    ## step3 : agent가 데이터를 축적하고 학습\n",
    "    player.save_experience() ## 데이터를 저장\n",
    "    player.learn() # 저장된 데이터를 학습\n",
    "\n",
    "    ##---강화학습의 종료 시점 결정---##\n",
    "    if (player.n_experience >= 20) and (np.array(player.rewards)[-20:].mean() > 9.5) :\n",
    "        print(\"---게임 클리어---\")\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dab7a5-d385-4b3e-bdcb-09b92d25991e",
   "metadata": {},
   "source": [
    "`-` Bandit 이동경로 시각화\n",
    "\n",
    "```Python\n",
    "def show(states: List[int]) -> matplotlib.pyplot.Figure :\n",
    "    \"\"\"\n",
    "    2차원 점들의 집합을 넣으면 빨간 점으로 이동 경로를 시각화하는 함수\n",
    "    \"\"\"\n",
    "    fig = plt.Figure()\n",
    "    ax = fig.subplots()\n",
    "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
    "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
    "    ax.text(0, 0, 'start', ha='center', va='center')\n",
    "    ax.text(3, 3, 'end', ha='center', va='center')\n",
    "    # Adding grid lines to the plot\n",
    "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
    "    \n",
    "    def update(t):\n",
    "        if states[t] in state_space:\n",
    "            s1,s2 = states[t]\n",
    "            states[t] = [s2,s1]\n",
    "            sc.set_offsets(states[t])\n",
    "        else:\n",
    "            s1,s2 = states[t]\n",
    "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
    "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
    "            states[t] = [s2,s1]       \n",
    "            sc.set_offsets(states[t])\n",
    "            \n",
    "    ani = FuncAnimation(fig,update,frames=len(states))\n",
    "    display(IPython.display.HTML(ani.to_jshtml()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dcbd3-edc7-4865-87a2-0663825e737e",
   "metadata": {},
   "source": [
    "`-` 4x4 GridWorld Environment\n",
    "\n",
    "* 핵심 함수 : `gym.spaces.Discrete()`\n",
    "\n",
    "```Python\n",
    "class GridWorld :\n",
    "    def __init__(self) :\n",
    "        self.a2d = {\n",
    "            0 : np.array([0, 1]),\n",
    "            1 : np.array([0, -1]),\n",
    "            2 : np.array([1, 0]),\n",
    "            3 : np.array([-1, 0])\n",
    "        }\n",
    "\n",
    "        self.state = np.array([0, 0])\n",
    "        self.state_space = gym.spaces.MultiDiscrete([4, 4])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "\n",
    "    def reset(self) :\n",
    "        self.state = np.array([0, 0])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action) :\n",
    "        self.state += self.a2d[action]\n",
    "        s1, s2 = self.state\n",
    "\n",
    "        if (s1 == 3) and (s2 == 3) :\n",
    "            self.reward = 100\n",
    "            self.terminated = True\n",
    "        \n",
    "        elif self.state in self.state_space :\n",
    "            self.reward = -1\n",
    "            self.terminated = False\n",
    "\n",
    "        else :\n",
    "            self.reward = -10\n",
    "            self.terminated = True\n",
    "\n",
    "        return self.state, self.reward, self.terminated\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e00b5d-e5e8-4172-87a4-b361e45a4aa1",
   "metadata": {},
   "source": [
    "## 3. Bandit 환경 설계 및 풀이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c10472-6462-4abf-87e9-db28f8f473f7",
   "metadata": {},
   "source": [
    "### **A. 대충 개념만 실습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c09eb32-b9ff-4152-bb7c-9da67f4b753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0,1] \n",
    "actions_deque = collections.deque(maxlen=500)\n",
    "rewards_deque =  collections.deque(maxlen=500)\n",
    "#---#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e272cedc-3435-472f-ad3d-ec159c148ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    action = np.random.choice(action_space)\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2fa561-de54-444d-96f2-f606f77655d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([np.int64(1),\n",
       "       np.int64(1),\n",
       "       np.int64(0),\n",
       "       np.int64(0),\n",
       "       np.int64(1),\n",
       "       np.int64(1),\n",
       "       np.int64(1),\n",
       "       np.int64(0),\n",
       "       np.int64(1),\n",
       "       np.int64(1)],\n",
       "      maxlen=500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff44f3de-fe98-4d8f-9259-178bb415665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([10, 10, 1, 1, 10, 10, 10, 1, 10, 10], maxlen=500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbb9ad8-9c18-4335-8e81-3be6a9339e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_numpy = np.array(actions_deque)\n",
    "rewards_numpy = np.array(rewards_deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f55550c-25e2-450e-869c-3b37a2d85057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., 10.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "q_table = np.array([q0,q1])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ff50dd-9d43-4ca4-a284-06c21de2d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = q_table.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6fa819c-7929-4c24-a3b8-30daa9e3670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    action = q_table.argmax()\n",
    "    if action == 1:\n",
    "        reward = 10 \n",
    "    else:\n",
    "        reward = 1\n",
    "    actions_deque.append(action)\n",
    "    rewards_deque.append(reward)\n",
    "    actions_numpy = np.array(actions_deque)\n",
    "    rewards_numpy = np.array(rewards_deque)    \n",
    "    q0 = rewards_numpy[actions_numpy == 0].mean()\n",
    "    q1 = rewards_numpy[actions_numpy == 1].mean()\n",
    "    q_table = np.array([q0,q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f773084-c08c-46d3-a50c-4b8487538917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b73301a3-adfd-4f78-8655-870c10f8df8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  1,  1, 10, 10, 10,  1, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbae39-7872-412d-9af3-5b2780e652da",
   "metadata": {},
   "source": [
    "### **B. 클래스를 이용한 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fdcf4b-3e6e-451f-b912-9fc5bb2e986f",
   "metadata": {},
   "source": [
    "```Python\n",
    "class Bandit:\n",
    "    def __init__(self):\n",
    "        self.reward = None \n",
    "    def step(self,action):\n",
    "        if action == 0:\n",
    "            self.reward = 1\n",
    "        else: \n",
    "            self.reward = 10 \n",
    "        return self.reward \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0efe5f-ded5-4b20-b192-e0cff77932b4",
   "metadata": {},
   "source": [
    "```Python\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def act(self):\n",
    "        # 만약에 경험이 20보다 작음 --> 랜덤액션 \n",
    "        # 경험이 20보다 크면 --> action = q_tabel.argmax()\n",
    "        pass \n",
    "    def save_experience(self):\n",
    "        # 데이터 저장 \n",
    "        pass \n",
    "    def learn(self):\n",
    "        # q_table 을 업데이트하는 과정 \n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944cd1e-6ae5-4340-b7e6-d0e67312945a",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b54d1b-a6b6-4cec-90f4-3b0d50f77973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit :\n",
    "    def __init__(self) :\n",
    "        self.reward = None\n",
    "    def step(self, action) :\n",
    "        if action == 0 :\n",
    "            self.reward = 1\n",
    "        else :\n",
    "            self.reward = 10\n",
    "        return self.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f768b-ce6b-4969-b5f2-4bbac65449c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent :\n",
    "    def __init__(self) :\n",
    "        self.n_experience = 0\n",
    "        \n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.actions = collections.deque(maxlen = 500)\n",
    "        self.rewards = collections.deque(maxlen = 500)\n",
    "        \n",
    "        self.action_space = [0, 1]\n",
    "        self.q_table = None\n",
    "        \n",
    "        \n",
    "    def act(self) :\n",
    "        if self.n_experience < 20 :\n",
    "            self.action = np.random.choice(self.action_space)\n",
    "        else :\n",
    "            self.action = self.q_table.argmax()\n",
    "\n",
    "        print(f\"버튼 {self.action} 누름\")\n",
    "\n",
    "            \n",
    "    def save_experience(self) :\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.n_experience += 1\n",
    "\n",
    "        \n",
    "    def learn(self) :\n",
    "        if self.n_experience < 20 :\n",
    "            pass\n",
    "        else :\n",
    "            actions = np.array(self.actions)\n",
    "            rewards = np.array(self.rewards)\n",
    "            q0 = rewards[actions == 0].mean()\n",
    "            q1 = rewards[actions != 0].mean()\n",
    "    \n",
    "            self.q_table = np.array([q0, q1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf9ecd-41ac-4415-bba1-1c712e322e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()\n",
    "player = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570c0a42-abd5-46f7-aac7-b1b2daba24fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "player.act()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5f8ce-976b-42a8-ab93-8bc38d90ba2f",
   "metadata": {},
   "source": [
    "`-` 기본 템플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958b2c1-da7b-4845-9abe-8dc5d7a7199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100) :\n",
    "    ## step1 : agent action\n",
    "    player.act()\n",
    "\n",
    "    ## step2 : action -> state, reward\n",
    "    player.reward = env.step(player.action)\n",
    "\n",
    "    ## step3 : agent가 데이터를 축적하고 학습\n",
    "    player.save_experience() ## 데이터를 저장\n",
    "    player.learn() # 저장된 데이터를 학습\n",
    "\n",
    "    ##---강화학습의 종료 시점 결정---##\n",
    "    if (player.n_experience >= 20) and (np.array(player.rewards)[-20:].mean() > 9.5) :\n",
    "        print(\"---게임 클리어---\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c72a1-0398-4c76-a677-428fc686dfb5",
   "metadata": {},
   "source": [
    "## 4. 예비학습 : `gym.spaces`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c974cf61-b4d8-4e8b-9355-ac35fb097bd7",
   "metadata": {},
   "source": [
    "`-` 예시 1 : 샘플링 및 집합 기능 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a2ed0-26f8-4abc-80ac-d53c0da3e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)\n",
    "action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d72b8a-d823-4d6a-ad49-d4d4d7746214",
   "metadata": {},
   "outputs": [],
   "source": [
    "[action_space.sample() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe522a-d2e8-4908-b01a-6df5a6e8355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "0 in action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e3428-3f8b-47c8-89cf-e0b472083e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "4 in action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547924b-9b4f-4fef-ad70-962bb3aa50b0",
   "metadata": {},
   "source": [
    "`-` 예시2 : n차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f395f-f5fb-427b-9a63-4133a7b837c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = gym.spaces.MultiDiscrete([4, 4])\n",
    "state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861469f4-6575-4b6c-afb0-0fa4737e9b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[state_space.sample() for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9492d00-4d87-442e-aa74-1df3e5bc5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "[0, 1] in state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6deb26-51c9-4515-806d-daa35525869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3, 3]) in state_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5e542-64fa-4789-9293-95f6d68b1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([3, 4]) in state_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6181aa37-ecde-464a-813a-18291e37417e",
   "metadata": {},
   "source": [
    "## 5. 4x4 Grid World 게임 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b23f9e-ef4b-4f09-aa42-59426391bdc9",
   "metadata": {},
   "source": [
    "### **A. 게임 설명**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cec9ae-4f24-41cf-b6a4-7b79cd0ffee3",
   "metadata": {},
   "source": [
    "`-` 4x4 그리드 월드에서 상하좌우로 움직이는 에이전트가 목표점에 도달하도록 하는 게임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6944902-f17b-4249-8f11-2d0a9c002483",
   "metadata": {},
   "source": [
    "`-` GridWorld에서 사용되는 주요변수\n",
    "\n",
    "    1. State : 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중 하나에 있을 수 있음\n",
    "    2. Action : 에이전트는 현재상태에서 다음상태로 이동하기 위해 상/하/좌/우 중 하나의 행동을 취할 수 있음\n",
    "    3. Reward : 에이전트가 현재상태에서 특정 action을 하면 얻어지는 보상.\n",
    "    4. Terminated : 하나의 에피소드가 종료되었음을 나타내는 상태."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119a374-cfde-4c6f-b5aa-a494f721c8c6",
   "metadata": {},
   "source": [
    "`-` 에이전트 환경\n",
    "\n",
    "* 에이전트 행동 : 상하좌우로 이동 - 4개의 행동 == `[0, 1, 2, 3]`\n",
    "* 환경은 보상을 줌 : `-1, -10, +100`\n",
    "  * `-1` : 격자 안에 에이전트가 있음 `&` 에이전트의 위치가 `(3, 3)`이 아님\n",
    "  * `+100` : 에이전트의 위치가 `(3, 3)`\n",
    "  * `-10` : 에이전트가 격자안에 있지 않음\n",
    "* 에이전트 (Action) <---> 환경 (State, Reward, Terminated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780c461-c24d-4acc-9521-33db25a74a85",
   "metadata": {},
   "source": [
    "### **B. 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68439fa2-968e-49d5-b5d1-a02b74e7dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(states):\n",
    "    \"\"\"\n",
    "    빨간 점으로 이동 경로를 시각화하는 함수\n",
    "    \"\"\"\n",
    "    fig = plt.Figure()\n",
    "    ax = fig.subplots()\n",
    "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
    "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
    "    ax.text(0, 0, 'start', ha='center', va='center')\n",
    "    ax.text(3, 3, 'end', ha='center', va='center')\n",
    "    # Adding grid lines to the plot\n",
    "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
    "    \n",
    "    def update(t):\n",
    "        if states[t] in state_space:\n",
    "            s1,s2 = states[t]\n",
    "            states[t] = [s2,s1]\n",
    "            sc.set_offsets(states[t])\n",
    "        else:\n",
    "            s1,s2 = states[t]\n",
    "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
    "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
    "            states[t] = [s2,s1]       \n",
    "            sc.set_offsets(states[t])\n",
    "            \n",
    "    ani = FuncAnimation(fig,update,frames=len(states))\n",
    "    display(IPython.display.HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b4922-9367-4171-a024-1340fe87ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "show([[0,0],[1,0],[2,0],[3,0],[4,0]]) # show 사용방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbfa0c-6491-4468-be57-480358411b84",
   "metadata": {},
   "source": [
    "## 6. 4x4 Grid World 환경 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e40f2-e193-411a-a94e-c88a48627676",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = gym.spaces.Discrete(4)\n",
    "action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f9a56-382e-4c8f-ad0a-21f269476e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = gym.spaces.MultiDiscrete([4, 4])\n",
    "state_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e3ede-3920-41bc-8157-34c520c12cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = np.array([1, 1])\n",
    "state += np.array([0, 1])\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659106e0-884c-426c-a6cf-4de865370d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld :\n",
    "    def __init__(self) :\n",
    "        self.a2d = {\n",
    "            0 : np.array([0, 1]),\n",
    "            1 : np.array([0, -1]),\n",
    "            2 : np.array([1, 0]),\n",
    "            3 : np.array([-1, 0]),\n",
    "        }\n",
    "\n",
    "        self.state = np.array([0, 0])\n",
    "        self.state_space = gym.spaces.MultiDiscrete([4, 4])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "\n",
    "\n",
    "    def reset(self) :\n",
    "        self.state = np.array([0, 0])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def step(self, action) :\n",
    "        self.state += self.a2d[action]\n",
    "        s1, s2 = self.state\n",
    "        \n",
    "        if (s1 == 3) and (s2 == 3) :\n",
    "            self.reward = 100\n",
    "            self.terminated = True\n",
    "            \n",
    "        elif self.state in self.state_space :\n",
    "            self.reward = -1\n",
    "            \n",
    "        else :\n",
    "            self.reward = -10\n",
    "            self.terminated = True\n",
    "\n",
    "        print(\n",
    "            f\"action = {action}\\t\"\n",
    "            f\"state = {self.state - self.a2d[action]} -> {self.state}\\t\"\n",
    "            f\"reward = {self.reward}\\t\"\n",
    "            f\"termiated = {self.terminated}\"\n",
    "        )\n",
    "\n",
    "        return self.state, self.reward, self.terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b711cfe4-ac25-4099-9689-02961f41e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "340ef8e6-64f3-4a6d-94ee-43079488159f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action = 2\tstate = [0 0] -> [1 0]\treward = -1\ttermiated = False\n",
      "action = 3\tstate = [1 0] -> [0 0]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 0] -> [0 1]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [0 1] -> [0 0]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 0] -> [0 1]\treward = -1\ttermiated = False\n",
      "action = 2\tstate = [0 1] -> [1 1]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [1 1] -> [1 0]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [1 0] -> [1 1]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [1 1] -> [1 2]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [1 2] -> [1 1]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [1 1] -> [1 2]\treward = -1\ttermiated = False\n",
      "action = 3\tstate = [1 2] -> [0 2]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [0 2] -> [0 1]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [0 1] -> [0 0]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 0] -> [0 1]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 1] -> [0 2]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 2] -> [0 3]\treward = -1\ttermiated = False\n",
      "action = 1\tstate = [0 3] -> [0 2]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 2] -> [0 3]\treward = -1\ttermiated = False\n",
      "action = 0\tstate = [0 3] -> [0 4]\treward = -10\ttermiated = True\n"
     ]
    }
   ],
   "source": [
    "action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "for _ in range(50) :\n",
    "    action = action_space.sample()\n",
    "    \n",
    "    state, reward, terminated = env.step(action)\n",
    "\n",
    "    if terminated :\n",
    "        env.reset()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4361e0b-a637-4ecc-bf34-689118b7f3b5",
   "metadata": {},
   "source": [
    "## 7. \"에이전트 <-> 환경\" 상호작용 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139042e-ae97-43f4-858d-0085dfcefaed",
   "metadata": {},
   "source": [
    "`-` 우리가 구현하고 싶은 기능\n",
    "\n",
    "* `.act()` : 액션을 결정 -> 일단 어려우니 여기선 랜덤 액션\n",
    "* `.save_experience()` : 데이터를 저장 -> 해당 과정에 초점\n",
    "* `.learn()` : 데이터에서의 학습 -> 어차피 랜덤이니 여기선 pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c79020-b9fc-4a84-9a4d-9e374ac2227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent :\n",
    "    def __init__(self) :\n",
    "        self.state = np.array([0, 0])\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.next_state = None\n",
    "        self.terminated = None\n",
    "        #---# SARSA -> SARST\n",
    "        #---#\n",
    "        self.states = collections.deque(maxlen = 500)\n",
    "        self.actions = collections.deque(maxlen = 500)\n",
    "        self.rewards = collections.deque(maxlen = 500)\n",
    "        self.next_states = collections.deque(maxlen = 500)\n",
    "        self.terminations = collections.deque(maxlen = 500)\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.n_experience = 0\n",
    "        \n",
    "        \n",
    "    def act(self) :\n",
    "        self.action = self.action_space.sample()\n",
    "        \n",
    "            \n",
    "    def save_experience(self) :\n",
    "        self.states.append(self.state.copy()) ## 왜 나만 깊은복사 이슈가...\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(self.next_state.copy())\n",
    "        self.terminations.append(self.terminated)\n",
    "        \n",
    "        self.n_experience += 1\n",
    "\n",
    "        \n",
    "    def learn(self) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98704b3-af27-4716-8398-f7561632015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "player = RandomAgent()\n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d4a249-9108-47d4-9608-452a59171c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for e in range(1, 20) :\n",
    "    score = 0\n",
    "    player.state = env.reset()\n",
    "\n",
    "    for t in range(50) :\n",
    "        ## step 1 : Agent의 action\n",
    "        player.act()\n",
    "        ## step 2 : Env가 Agent의 action을 보고 next_state, reward, terminated를 전달\n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        ## step 3 : Agent가 save & learn\n",
    "        player.save_experience()\n",
    "        player.learn() ## pass\n",
    "        ## step 4 : next iteration으로의 이행\n",
    "        player.state = player.next_state\n",
    "        score += player.reward\n",
    "        if player.terminated :\n",
    "            print(f\"---에피소드{e}종료---\")\n",
    "            break\n",
    "\n",
    "    #---#\n",
    "    scores.append(score)\n",
    "    \n",
    "    if scores[-1] > 0 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13113905-11a6-429a-87e6-ae3b4ffb3c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d309b0f-0f36-4263-9b38-3dc3e815e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [np.array([0,0])]+ list(player.next_states)[-20:]\n",
    "show(paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
