{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **기말고사 내용 총 정리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "#---#\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 유용한 기능**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. 옵티마이저 고급**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 텐서를 그대로 파라미터로서 사용하는 법\n",
    "\n",
    "* `iterable`이기만 하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = torch.tensor(1.0, requires_grad = True)\n",
    "optimize = torch.optim.SGD([lamb], lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 시퀸스 없이 파라미터 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linr1 = torch.nn.Linear(1, 32)\n",
    "relu = torch.nn.ReLU()\n",
    "linr2 = torch.nn.Linear(32, 1)\n",
    "\n",
    "params = list(linr1.parameters()) + list(linr2.parameters())\n",
    "optimizr = torch.optim.Adam(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. gymnasium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 추천 시스템**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(df.x1.map({v:i for i, v in enumerate(set(df.x1))}))\n",
    "x2 = torch.tensor(df.x2.map({v:i for i, v in enumerate(set(df.x2))}))\n",
    "y = torch.tensor(df.y).reshape(-1, 1).float()\n",
    "\n",
    "X = torch.stack([x1, x2], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. MF-based 추천 시스템**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 클래스 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_recommend(torch.nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.ebdd1 = torch.nn.Embedding(9, 2)\n",
    "        self.ebdd2 = torch.nn.Embedding(8, 2)\n",
    "        self.bias1 = torch.nn.Embedding(9, 1)\n",
    "        self.bias2 = torch.nn.Embedding(8, 1)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, X) :\n",
    "        x1 = X[:, 0]\n",
    "        x2 = X[:, 1]\n",
    "\n",
    "        yhat = self.sig((self.ebdd1(x1) * self.ebdd2(x2)).sum(axis = 1).reshape(-1, 1) + self.bias1(x1) + self.bias2(x2))*5\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MF_recommend()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#---#\n",
    "for epoc in range(3000) :\n",
    "    yhat = net(X)\n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss.backward()\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = torch.tensor([0, 1, 8])\n",
    "xx2 = torch.tensor([0, 7, 2])\n",
    "XX = torch.stack([xx1, xx2], axis = 1)\n",
    "\n",
    "net(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. NN-based 추천 시스템**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNbased(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #--#\n",
    "        self.ebdd1 = torch.nn.Embedding(9,2)\n",
    "        self.ebdd2 = torch.nn.Embedding(8,2)\n",
    "        self.b1 = torch.nn.Embedding(9,1)\n",
    "        self.b2 = torch.nn.Embedding(8,1)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6,15),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(15,1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        x1 = X[:, 0]\n",
    "        x2 = X[:, 1]\n",
    "        \n",
    "        W_feature = self.ebdd1(x1)\n",
    "        W_bias = self.b1(x1)\n",
    "        M_feature = self.ebdd2(x2)\n",
    "        M_bias = self.b2(x2)\n",
    "        \n",
    "        Z = torch.concat([W_feature, M_feature, W_bias, M_bias],axis=1)\n",
    "        \n",
    "        yhat = self.mlp(Z) * 5 \n",
    "        \n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NNbased()\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters(),lr=0.1)\n",
    "#--# \n",
    "for epoc in range(3000):\n",
    "    yhat = net(X)\n",
    "    \n",
    "    loss = loss_fn(yhat,y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = torch.tensor([0, 1, 8])\n",
    "xx2 = torch.tensor([0, 7, 2])\n",
    "XX = torch.stack([xx1, xx2], axis = 1)\n",
    "\n",
    "net(XX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 순환 신경망**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. hidden feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` $h$는 사실 문자열을 숫자로 바꾼 표현으로 해석할 수 있음. 즉, 원-핫 인코딩과 다른 형태의 숫자표현으로 해석 가능.\n",
    "\n",
    "* `hidden feature`로서 작용함 -> 학습시켜 생산된 피쳐\n",
    "\n",
    "`-` 사실 $h$는 원-핫 인코딩보다 약간 더 (1) 액기스만 남은 느낌 + (2) 숙성된 느낌을 준다.\n",
    "\n",
    "* (why 1) $h$는 $x$보다 $y$를 예측함에 좀 더 직접적인 역할을 한다. 즉, $x$ 숫자보다 $h$ 숫자가 잘 정리되어 있고(차원이 낮고), 입력의 특징을 잘 정리한 의미있는 숫자이다.\n",
    "* (why 2) $x$는 학습없이 그냥 얻어지는 숫자표현이지만, $h$는 학습을 통하여 고치고 고치고 고치고 고친 숫자표현이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. 하이퍼볼릭 탄젠트 시그모이드**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `torch.nn.Tanh()`\n",
    "\n",
    "* 실수를 `(-1, 1)`의 값으로 변환\n",
    "* 기존 로지스틱 시그모이드의 경우 그 값이 0에 가까워질 수 있어 `weight`에 둔감할 수 있음 -> 단점 해소\n",
    "* 여러 개의 은닉 노드를 사용하여 변환을 더욱 다양화할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C. RNN 차원 표기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X.shape == $(L, H_{\\text{in}})$ : 문자 길이 / 고유 문자열 수\n",
    "* h.shape == $(L, H_{\\text{out}})$\n",
    "* y.shape == $(L, Q)$ : 문자 길이 / 고유 문자열 수\n",
    "* Xt.shape == $(H_{\\text{in}},)$\n",
    "* ht.shape == $(H_{\\text{out}},)$\n",
    "* yt.shape == $(Q,)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D. rNNCell class 생성**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rNNCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.i2h = torch.nn.Linear(4,2)\n",
    "        self.h2h = torch.nn.Linear(2,2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self,Xt,ht): # 엄밀하게는 h_{t-1}\n",
    "        ht = self.tanh(self.i2h(Xt) + self.h2h(ht)) \n",
    "        \n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnncell = rNNCell()\n",
    "cook = torch.nn.Linear(2,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(\n",
    "    list(rnncell.parameters()) + \n",
    "    list(cook.parameters()),\n",
    "    lr=0.1\n",
    ")\n",
    "\n",
    "#---#\n",
    "L = len(X)\n",
    "for epoc in range(200):\n",
    "    # 1~2 \n",
    "    loss = 0\n",
    "    ht = torch.zeros(2) # 첫 ht는 맹물\n",
    "    for t in range(L):\n",
    "        Xt, yt = X[t], y[t]\n",
    "        ht = rnncell(Xt,ht)\n",
    "        ot = cook(ht)\n",
    "        loss = loss_fn(ot,yt) + loss \n",
    "    loss = loss / L \n",
    "    # 3\n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(L,2) ## 공간 생성\n",
    "h0 = torch.zeros(2) ## 초기값 : h0\n",
    "h[0] = rnncell(X[0], h0) ## h1\n",
    "\n",
    "for t in range(1,L):\n",
    "    h[t] = rnncell(X[t], h[t-1])\n",
    "\n",
    "yhat = torch.nn.functional.softmax(cook(h),dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E. RNNCell 모듈 사용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 동일한 방식으로 `rNNCell` 클래스를 설계했기 때문에 사용법은 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnncell = torch.nn.RNNCell(\n",
    "    input_size = 4, #X.shape = (L,4)\n",
    "    hidden_size= 2, # h.shape = (L,2) \n",
    ")\n",
    "cook = torch.nn.Linear(2,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(\n",
    "    list(rnncell.parameters()) + \n",
    "    list(cook.parameters()),\n",
    "    lr=0.1\n",
    ")\n",
    "#---#\n",
    "L = len(X)\n",
    "for epoc in range(200):\n",
    "    # 1~2 \n",
    "    loss = 0\n",
    "    ht = torch.zeros(2) # 첫 ht는 맹물\n",
    "    for t in range(L):\n",
    "        Xt, yt = X[t], y[t]\n",
    "        ht = rnncell(Xt,ht)\n",
    "        ot = cook(ht)\n",
    "        loss = loss_fn(ot,yt) + loss \n",
    "    loss = loss / L \n",
    "    # 3\n",
    "    loss.backward()\n",
    "    # 4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.zeros(L,2) ## 공간 생성\n",
    "h0 = torch.zeros(2) ## 초기값 : h0\n",
    "h[0] = rnncell(X[0], h0) ## h1\n",
    "\n",
    "for t in range(1,L):\n",
    "    h[t] = rnncell(X[t], h[t-1])\n",
    "\n",
    "yhat = torch.nn.functional.softmax(cook(h),dim=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 파라미터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "rnncell = torch.nn.RNNCell(4,2)\n",
    "rnncell.weight_ih.data = _rnncell.i2h.weight.data\n",
    "rnncell.weight_hh.data =  _rnncell.h2h.weight.data\n",
    "rnncell.bias_ih.data = _rnncell.i2h.bias.data\n",
    "rnncell.bias_hh.data = _rnncell.h2h.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F. RNN 모듈**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(\n",
    "    input_size= 4, # X.shape=(L,H_in)\n",
    "    hidden_size = 2, # h.shape=(L,H_out)\n",
    "    num_layers = 1, # 어느 방향에서 레이어가 들어오는지의 차원. 피쳐가 몇개인지 느낌? 디폴트는 1\n",
    "    bidirectional = False # bi-directional : True이면 뒤에서부터도 읽을 수 있는 양방향 네트워크. 디폴트는 False\n",
    ")\n",
    "cook = torch.nn.Linear(2,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(list(rnn.parameters())+list(cook.parameters()),lr=0.1)\n",
    "\n",
    "#---#\n",
    "for epoc in range(200) :\n",
    "    ## 초기화 하는 경우\n",
    "    # h0 = torch.zeros(1, 2) ## D*num_layers == 1*1\n",
    "    # h, hL = rnn(X, h0) ## (h, h[L])\n",
    "    h, hL = rnn(X) ## 초기화 안하고 그냥 넣어도 돌아감\n",
    "    netout = cook(h)\n",
    "    \n",
    "    loss = loss_fn(netout, y)\n",
    "    loss.backward()\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, hL = rnn(X)\n",
    "yhat = torch.nn.functional.softmax(cook(h),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 강화 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 마음에 들지 않지만 꼭 외워야 하는것\n",
    ">\n",
    "> 1.  `env.step`은 항상 next_state, reward, terminated, truncated, info\n",
    ">     를 리턴한다. – 짐나지엄 라이브러리 규격때문\n",
    "> 2.  `env.reset`은 환경을 초기화할 뿐만 아니라, state, info를 반환하는\n",
    ">     기능도 있다. – 짐나지엄 라이브러리 규격때문\n",
    "> 3.  `player`는 항상 `state`와 `next_state`를 구분해서 저장한다.\n",
    ">     (다른변수들은 그렇지 않음) 이는 강화학습이\n",
    ">     MDP(마코프체인+행동+보상)구조를 따르기 때문에 생기는 고유한\n",
    ">     특징이다 – 이론적인 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. `q_table`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` **Bandit Game**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit :\n",
    "    def __init__(self) :\n",
    "        self.reward = None\n",
    "\n",
    "    def step(self, action) :\n",
    "        if action == 0 :\n",
    "            self.reward = 1\n",
    "        else :\n",
    "            self.reward = 10\n",
    "\n",
    "        return self.reward\n",
    "\n",
    "\n",
    "class Agent :\n",
    "    def __init__(self) :\n",
    "        self.n_experience = 0\n",
    "\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.actions = collections.deque(maxlen = 500)\n",
    "        self.rewards = collections.deque(maxlen = 500)\n",
    "\n",
    "        self.action_space = [0, 1]\n",
    "        self.q_table = None\n",
    "\n",
    "    def act(self) :\n",
    "        if self.n_experience < 20 :\n",
    "            self.action = np.random.choice(self.action_space)\n",
    "        else :\n",
    "            self.action = self.q_table.argmax()\n",
    "\n",
    "    def save_experience(self) :\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.n_experience += 1\n",
    "\n",
    "    def learn(self) :\n",
    "        actions = np.array(self.actions)\n",
    "        rewards = np.array(self.rewards)\n",
    "\n",
    "        q0 = rewards[actions == 0].mean()\n",
    "        q1 = rewards[actions == 1].mean()\n",
    "\n",
    "        self.q_table = np.array([q0, q1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Bandit()\n",
    "player = Agent()\n",
    "\n",
    "for _ in range(100) :\n",
    "    player.act()\n",
    "    player.reward = env.step(player.action)\n",
    "    player.save_experience()\n",
    "    player.learn()\n",
    "\n",
    "    if (player.n_experience >= 20) and (np.array(player.rewards)[-20:].mean() > 9.5) :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 4x4 Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld :\n",
    "    def __init__(self) :\n",
    "        self.a2d = {\n",
    "            0: np.array([0,1]),  # →\n",
    "            1: np.array([0,-1]), # ←  \n",
    "            2: np.array([1,0]),  # ↓\n",
    "            3: np.array([-1,0])  # ↑\n",
    "        }\n",
    "\n",
    "        self.states_space = gym.spaces.MultiDiscrete([4, 4])\n",
    "        self.state = np.array([0, 0])\n",
    "        self.reward = None\n",
    "        self.terminated = False\n",
    "\n",
    "    def step(self, action) :\n",
    "        self.state = self.state + self.a2d[action] ## 여기서 깊은복사 이슈 나는듯\n",
    "        s1, s2 = self.state\n",
    "\n",
    "        if (s1 == 3) and (s2 == 3) :\n",
    "            self.reward = 100\n",
    "            self.terminated = True\n",
    "        \n",
    "        elif self.state in self.states_space :\n",
    "            self.reward = -1\n",
    "\n",
    "        else :\n",
    "            self.reward = -10\n",
    "            self.terminated = True\n",
    "\n",
    "        return self.state, self.reward, self.terminated\n",
    "\n",
    "    def reset(self) :\n",
    "        self.state = np.array([0, 0])\n",
    "        self.terminated = False\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "\n",
    "class RandomAgent :\n",
    "    def __init__(self) :\n",
    "        self.state = np.array([0, 0])\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.next_state = None\n",
    "        self.terminated = None\n",
    "\n",
    "        self.states = collections.deque(maxlen = 500000)\n",
    "        self.actions = collections.deque(maxlen = 500000)\n",
    "        self.rewards = collections.deque(maxlen = 500000)\n",
    "        self.next_states = collections.deque(maxlen = 500000)\n",
    "        self.terminations = collections.deque(maxlen = 500000)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "        self.n_experience = 0\n",
    "\n",
    "    def act(self) :\n",
    "        self.action = self.action_space.sample()\n",
    "\n",
    "    def save_experience(self) :\n",
    "        self.states.append(self.state)\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(self.next_state)\n",
    "        self.terminations.append(self.terminated)\n",
    "        self.n_experience += 1\n",
    "\n",
    "    def learn(self) :\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = RandomAgent()\n",
    "env = GridWorld()\n",
    "scores = [] \n",
    "score = 0 \n",
    "#\n",
    "for e in range(1,100000):\n",
    "    #---에피소드시작---#\n",
    "    while True:\n",
    "        # step1 -- 액션선택\n",
    "        player.act()\n",
    "        # step2 -- 환경반응 \n",
    "        player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "        # step3 -- 경험기록 & 학습 \n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        # step4 --종료 조건 체크 & 후속 처리\n",
    "        if env.terminated:\n",
    "            score = score + player.reward\n",
    "            scores.append(score)\n",
    "            score = 0 \n",
    "            player.state = env.reset() \n",
    "            break\n",
    "        else: \n",
    "            score = score + player.reward\n",
    "            scores.append(score)            \n",
    "            player.state = player.next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 실시간 학습이 아닌 전부 행동한 뒤에 학습하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 평균을 이용한 `q_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([4, 4, 4])\n",
    "count = np.zeros([4, 4, 4])\n",
    "\n",
    "for (s1, s2), a, r in zip(player.states, player.actions, player.rewards) :\n",
    "    q_table[s1, s2, a] += r\n",
    "    count[s1, s2, a] += 1\n",
    "\n",
    "count[count == 0] = 1e-6\n",
    "q_table = q_table/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 부스팅 기법을 이용한 `q_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([4, 4, 4])\n",
    "\n",
    "for (s1, s2), a, r in zip(player.states, player.actions, player.rewards) :\n",
    "    qhat = q_table[s1, s2, a]\n",
    "    q = r\n",
    "    diff = qhat - q\n",
    "    q_table[s1, s2, a] += 0.01*diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 감가율(Discount Rate)을 적용한 `q_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(player,s1,s2):\n",
    "    action = player.q_table[s1,s2,:].argmax()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1, 50) :\n",
    "    s1, s2 = player.state\n",
    "    action = act(s1, s2)\n",
    "    player.next_state, player.reward, player.terminated = env.step(action)\n",
    "    player.save_experience()\n",
    "    player.learn()\n",
    "    score += player.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 개선된 Agent : Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 즉시 얻게되는 보상과 미래에 얻게되리라 기대되는 보상을 동일하게 취급할 수는 없음\n",
    "* 모든 $(s, a)$에 대하여 $Q(s, a)$의 값은 아래와 같이 정의하는 게 합리적이다.\n",
    "\n",
    "$$Q(s, a) = R(s, a) + \\gamma \\max_{a'} Q(s', a')$$\n",
    "\n",
    "> 미래의 상황 중 가장 좋은 선택에 대해 감가율을 적용하여 더함.\n",
    "> \n",
    "> **Note**\n",
    ">\n",
    "> 사실 좀 더 실용적으로는(=코딩친화적으로는) 아래의 수식을 쓰는게 좋다.\n",
    ">\n",
    "> $$Q(s,a) = \\begin{cases}  R(s,a) & \\text{if terminated} \\\\ R(s,a) + \\gamma \\max_{a'}Q(s',a') & \\text{else} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((4,4,4))\n",
    "memory = zip(player.states, player.actions, player.rewards, player.next_states, player.terminations)\n",
    "\n",
    "for (s1,s2), a, r, (ss1, ss2), tmd in memory:\n",
    "    qhat = q_table[s1,s2,a] # 내가 생각했던값\n",
    "    \n",
    "    if tmd :\n",
    "        q = r # 실제값\n",
    "    else :\n",
    "        future = q_table[ss1, ss2, :].max()\n",
    "        q = r + 0.95*future\n",
    "        \n",
    "    diff = q-qhat # 차이\n",
    "    q_table[s1,s2,a] = q_table[s1,s2,a] + 0.01*diff# update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `q_table`의 이용 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(states):\n",
    "    fig = plt.Figure()\n",
    "    ax = fig.subplots()\n",
    "    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n",
    "    sc = ax.scatter(0, 0, color='red', s=500)  \n",
    "    ax.text(0, 0, 'start', ha='center', va='center')\n",
    "    ax.text(3, 3, 'end', ha='center', va='center')\n",
    "    # Adding grid lines to the plot\n",
    "    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n",
    "    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n",
    "    state_space = gym.spaces.MultiDiscrete([4,4])\n",
    "    def update(t):\n",
    "        if states[t] in state_space:\n",
    "            s1,s2 = states[t]\n",
    "            states[t] = [s2,s1]\n",
    "            sc.set_offsets(states[t])\n",
    "        else:\n",
    "            s1,s2 = states[t]\n",
    "            s1 = s1 + 0.5 if s1 < 0 else (s1 - 0.5 if s1 > 3 else s1)\n",
    "            s2 = s2 + 0.5 if s2 < 0 else (s2 - 0.5 if s2 > 3 else s2)\n",
    "            states[t] = [s2,s1]       \n",
    "            sc.set_offsets(states[t])\n",
    "    ani = FuncAnimation(fig,update,frames=len(states))\n",
    "    display(IPython.display.HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.state = env.reset()\n",
    "player.terminated = False\n",
    "score = 0\n",
    "\n",
    "for t in range(2**8) :\n",
    "    ## step1 --- 액션 선택\n",
    "    s1, s2 = player.state\n",
    "    player.action = act(player, s1, s2)\n",
    "    ## step2 --- 환경 반응\n",
    "    player.next_state, player.reward, player.terminated = env.step(player.action)\n",
    "    ## step3 --- 경헙기록 & 학습\n",
    "    player.save_experience()\n",
    "    player.learn()\n",
    "    ## step4 --- 종료 조건 체크\n",
    "    if env.terminated :\n",
    "        score += player.reward\n",
    "        player.state = env.reset()\n",
    "        player.playtimes.append(t+1)\n",
    "        break\n",
    "    else :\n",
    "        score += player.reward\n",
    "        player.state = player.next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.concat([np.array([[0, 0]]), np.array(player.next_states)[-player.playtimes[-1]:]], axis = 0)\n",
    "show(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. `q_net`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 전략 : `q_table`에 대응하는 뭔가를 만들자\n",
    "\n",
    "* 상태공간에서 가능한 행동이 한정되어있고, 매우 많지 않은 경우 모든 경우의 수를 조사 가능\n",
    "* 아주 많은 경우에 대한 reward 값을 조사하는 것은 현실적으로 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 클래스 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent :\n",
    "    def __init__(self) :\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        self.reward = None\n",
    "        self.next_state = None\n",
    "        self.terminated = None\n",
    "        # self.truncated = None ## 얜 필요 없음??\n",
    "\n",
    "        self.states = collections.deque(maxlen = 5000)\n",
    "        self.actions = collections.deque(maxlen = 5000)\n",
    "        self.rewards = collections.deque(maxlen = 5000)\n",
    "        self.next_states = collections.deque(maxlen = 5000)\n",
    "        self.terminations = collections.deque(maxlen = 5000)\n",
    "        # self.truncations = collections.deque(maxlen = 5000) ## state에 들어가 있어서 학습에 영향 X긴 한데, 그럼 terminations도...\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4) ## env에도 있으나, 간단하므로 만듦\n",
    "        self.n_experiences = 0\n",
    "\n",
    "    def act(self) :\n",
    "        self.action = self.action_space.sample()\n",
    "\n",
    "    def save_experience(self) :\n",
    "        self.states.append(torch.tensor(self.state)) ## Pytorch 네트워크에 입력값으로 들어감\n",
    "        self.actions.append(self.action)\n",
    "        self.rewards.append(self.reward)\n",
    "        self.next_states.append(torch.tensor(self.next_state))\n",
    "        self.terminations.append(self.terminated)\n",
    "\n",
    "        self.n_experiences += 1\n",
    "\n",
    "    def learn(self) :\n",
    "        pass\n",
    "\n",
    "\n",
    "class Agent(RandomAgent) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.q_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(8, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 4)\n",
    "        )\n",
    "        self.optimizr = torch.optim.Adam(self.q_net.parameters())\n",
    "        self.eps = 1.0\n",
    "\n",
    "    def act(self) :\n",
    "        if random.random() < self.eps :\n",
    "            self.action = self.action_space.sample()\n",
    "        else :\n",
    "            state = torch.tensor(self.state)\n",
    "            self.action = self.q_net(state).argmax().item()\n",
    "            \n",
    "    def learn(self) :\n",
    "        if self.n_experiences > 64 :\n",
    "            for epoc in range(1) :\n",
    "                memory = list(zip(self.states, self.actions, self.rewards, self.next_states, self.terminations))\n",
    "                mini_batch = random.sample(memory, 64)\n",
    "            \n",
    "                ## step 1-2\n",
    "                loss = 0\n",
    "                \n",
    "                for s, a, r, ss, tmd in mini_batch :\n",
    "                    q_hat = self.q_net(s)[a]\n",
    "                    \n",
    "                    if tmd :\n",
    "                        q = r\n",
    "                    else :\n",
    "                        future = self.q_net(ss).max().data\n",
    "                        q = r + 0.99*future\n",
    "                        \n",
    "                    loss += (q_hat-q)**2\n",
    "                \n",
    "                loss = loss/64\n",
    "            \n",
    "                ## step 3\n",
    "                loss.backward()\n",
    "            \n",
    "                ## step 4\n",
    "                self.optimizr.step()\n",
    "                self.optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\")\n",
    "player = Agent()\n",
    "player.state, _ = env.reset()\n",
    "score = 0\n",
    "playtime = 0\n",
    "scores = []\n",
    "playtimes = []\n",
    "\n",
    "for e in range(1, 2001) :\n",
    "    ##-----에피소드 시작-----##\n",
    "    while True :\n",
    "        ## step 1\n",
    "        player.act()\n",
    "        ## step 2\n",
    "        player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n",
    "        ## step 3\n",
    "        player.save_experience()\n",
    "        player.learn()\n",
    "        ## step 4\n",
    "        score += player.reward\n",
    "        \n",
    "        if player.terminated or player.truncated :\n",
    "            scores.append(score)\n",
    "            playtimes.append(playtime)\n",
    "            score = 0\n",
    "            playtime = 0\n",
    "            player.state, _ = env.reset()\n",
    "            break\n",
    "\n",
    "        else :\n",
    "            playtime += 1\n",
    "            player.state = player.next_state\n",
    "\n",
    "    ##-----에피소드 끝-----##\n",
    "    player.eps = player.eps * 0.995\n",
    "    if (e % 50) == 0 :\n",
    "        print(\n",
    "            f\"에피소드: {e}\\t\",\n",
    "            f\"경험: {player.n_experiences}\\t\",\n",
    "            f\"점수(평균): {np.mean(scores[-100:]):.2f}\\t\",\n",
    "            f\"게임시간(평균): {np.mean(playtimes[-100:]):.2f}\\t\",\n",
    "            f\"돌발행동: {player.eps:.2f}\\t\",\n",
    "        )\n",
    "\n",
    "    if np.mean(scores[-100:]) > 200:\n",
    "        print(\"--루나랜더 클리어(2025.06.14.)--\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 아주 오래 걸리고, 불안정함. 안정적인 학습을 위해서는 복잡한 테크닉을 사용해야 합니다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs,jump=10):\n",
    "    imgs = imgs[::jump]\n",
    "    fig = plt.Figure()\n",
    "    ax = fig.subplots()\n",
    "    def update(i):\n",
    "        ax.imshow(imgs[i])\n",
    "    ani = FuncAnimation(fig,update,frames=len(imgs))\n",
    "    display(IPython.display.HTML(ani.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player.eps = 0\n",
    "\n",
    "while True:\n",
    "    player.act()\n",
    "    player.next_state, player.reward, player.terminated, player.truncated, _ = env.step(player.action)\n",
    "    imgs.append(env.render())\n",
    "    if player.terminated or player.truncated:\n",
    "        break\n",
    "    else:\n",
    "        player.state = player.next_state\n",
    "\n",
    "\n",
    "show(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. 용어 정리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. ${\\bf X}$, ${\\bf y}$**\n",
    "\n",
    "`-` X, y를 지칭하는 이름\n",
    "\n",
    "| 구분 | 용어                            | 설명                                                                                               |\n",
    "|----------|-----------------------------------------------------|----------|\n",
    "| X    | 설명변수                        | 종속변수(반응변수)를 설명하거나 예측하는 데 사용되는 변수로, 전통 통계 및 머신러닝에서의 입력 역할 |\n",
    "|      | 독립변수 (Independent Variable) | 전통적인 통계학 및 회귀 분석 문맥에서 사용됨                                                       |\n",
    "|      | 입력변수 (Input Variable)       | 머신러닝 모델에서 입력 데이터로 사용되며, 특히 신경망 구조 등에서 많이 쓰임                        |\n",
    "|      | 특징 / 특성 (Feature)           | 머신러닝, 데이터마이닝, 딥러닝 등에서 데이터를 구성하는 속성 또는 설명 변수로 사용됨               |\n",
    "|      | 예측 변수 (Predictor)           | 예측 모델 설계 시 독립변수를 지칭하는 용어로, 모델링/통계 분석 문맥에서 흔히 사용됨                |\n",
    "|      | 공변량 (Covariate)              | 실험 디자인, 특히 임상연구나 사회과학 연구에서 제어 변수로 사용됨                                  |\n",
    "| y    | 반응변수                        | 독립변수의 영향을 받는 결과 변수로, 모델링이나 인과 추론에서 핵심적인 대상                         |\n",
    "|      | 종속변수 (Dependent Variable)   | 전통 통계학과 회귀분석에서 사용되며, 독립변수의 영향을 받는 변수로 정의됨                          |\n",
    "|      | 출력변수 (Output Variable)      | 머신러닝 및 딥러닝에서 모델의 예측 결과로 출력되는 값으로 사용됨                                   |\n",
    "|      | 타겟 / 정답 (Target / Label)    | 지도학습에서 모델이 학습해야 하는 실제 정답값을 의미하며, 분류/회귀 문제에 공통적으로 사용됨       |\n",
    "\n",
    "`-` 더 다양함: <https://ko.wikipedia.org/wiki/독립변수와_종속변수>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. 지도학습**\n",
    "\n",
    "`-` 우리가 수업에서 다루는 데이터는 주로 아래와 같은 느낌이다.\n",
    "\n",
    "1.  데이터는 $(X,y)$의 형태로 정리되어 있다.\n",
    "\n",
    "2.  $y$는 우리가 관심이 있는 변수이다. 즉 우리는 $y$를 적절하게 추정하는\n",
    "    것에 관심이 있다.\n",
    "\n",
    "3.  $X$는 $y$를 추정하기 위해 필요한 정보이다.\n",
    "\n",
    "|           $X$           |           $y$            |        비고        |      순서      |               예시               |\n",
    "|:------------:|:--------------:|:----------:|:----------:|:------------------:|\n",
    "|       기온(온도)        | 아이스 아메리카노 판매량 |        회귀        |    상관없음    | 날씨가 판매량에 미치는 영향 분석 |\n",
    "|          스펙           |        합격 여부         |      로지스틱      |    상관없음    |     입사 지원자의 합격 예측      |\n",
    "|         이미지          |         카테고리         | 합성곱신경망 (CNN) |    상관없음    |      개/고양이 이미지 구분       |\n",
    "|    유저, 아이템 정보    |           평점           |     추천시스템     |    상관없음    |        넷플릭스 영화 추천        |\n",
    "| 처음 $m$개의 단어(문장) |  이후 1개의 단어(문장)   |  순환신경망 (RNN)  | 순서 상관 있음 |   챗봇, 문장 생성, 언어 모델링   |\n",
    "| 처음 $m$개의 단어(문장) |         카테고리         |  순환신경망 (RNN)  | 순서 상관 있음 |        영화리뷰 감정 분류        |\n",
    "\n",
    "`-` 이러한 문제상황, 즉 $(X,y)$가 주어졌을때 $X \\to y$를 추정하는 문제를\n",
    "supervised learning 이라한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C. 모델이란?**\n",
    "\n",
    "`-` 통계학에서 모델은 y와 x의 관계를 의미하며 오차항의 설계를 포함하는\n",
    "개념이다. 이는 통계학이 “데이터 = 정보 + 오차”의 관점을 유지하기\n",
    "때문이다. 따라서 통계학에서 모델링이란\n",
    "\n",
    "$$y_i = net(x_i) + \\epsilon_i$$\n",
    "\n",
    "에서 (1) 적절한 함수 $net$를 선택하는 일 (2) 적절한 오차항 $\\epsilon_i$\n",
    "을 설계하는일 모두를 포함한다.\n",
    "\n",
    "`-` 딥러닝 혹은 머신러닝에서 모델은 단순히\n",
    "\n",
    "$$y_i \\approx net(x_i)$$\n",
    "\n",
    "를 의미하는 경우가 많다. 즉 “model=net”라고 생각해도 무방하다. 이 경우\n",
    "“모델링”이란 단순히 적절한 $net$을 설계하는 것만을 의미할 경우가 많다.\n",
    "\n",
    "`-` 그래서 생긴일\n",
    "\n",
    "-   통계학교재 특징: 분류문제와 회귀문제를 엄밀하게 구분하지 않는다.\n",
    "    사실 오차항만 다를뿐이지 크게보면 같은 회귀모형이라는 관점이다.\n",
    "    그래서 일반화선형모형(GLM)이라는 용어를 쓴다.\n",
    "-   머신러닝/딥러닝교재 특징: 회귀문제와 분류문제를 구분해서 설명한다.\n",
    "    (표도 만듦) 이는 오차항에 대한 기술을 모호하게 하여 생기는 현상이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D. 학습이란?**\n",
    "\n",
    "`-` 학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
    "“규칙” 혹은 “원리”를 찾는 것이다.\n",
    "\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
    "    “맵핑”을 찾는 것이다.\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
    "    “함수”을 찾는 것이다. 즉 $y\\approx f(X)$가 되도록 만드는 $f$를 잘\n",
    "    찾는 것이다. (이 경우 “함수를 추정한다”라고 표현)\n",
    "-   학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는 어떠한\n",
    "    “모델” 혹은 “모형”을 찾는 것이다. 즉 $y\\approx model(X)$가 되도록\n",
    "    만드는 $model$을 잘 찾는 것이다. (이 경우 “모형을 학습시킨다”라고\n",
    "    표현)\n",
    "-   **학습이란 주어진 자료 $(X,y)$를 잘 분석하여 $X$에서 $y$로 가는\n",
    "    어떠한 “네트워크”을 찾는 것이다. 즉 $y\\approx net(X)$가 되도록\n",
    "    만드는 $net$을 잘 찾는 것이다. (이 경우 “네트워크를 학습시킨다”라고\n",
    "    표현)**\n",
    "\n",
    "`-` prediction이란 학습과정에서 찾은 “규칙” 혹은 “원리”를 $X$에 적용하여\n",
    "$\\hat{y}$을 구하는 과정이다. 학습과정에서 찾은 규칙 혹은 원리는\n",
    "$f$,$model$,$net$ 으로 생각가능한데 이에 따르면 아래가 성립한다.\n",
    "\n",
    "-   $\\hat{y} = f(X)$\n",
    "-   $\\hat{y} = model(X)$\n",
    "-   $\\hat{y} = net(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **E. $\\hat{y}$를 부르는 다양한 이름**\n",
    "\n",
    "`-` $\\hat{y}$는 $X$가 주어진 자료에 있는 값인지 아니면 새로운 값 인지에\n",
    "따라 지칭하는 이름이 미묘하게 다르다.\n",
    "\n",
    "1.  $X \\in data$: $\\hat{y}=net(X)$ 는 predicted value, fitted value 라고\n",
    "    부른다.\n",
    "\n",
    "2.  $X \\notin data$: $\\hat{y}=net(X)$ 는 predicted value, predicted\n",
    "    value with new data 라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **F. 다양한 코드들**\n",
    "\n",
    "`-` 파이썬 코드..\n",
    "\n",
    "``` python\n",
    "#Python\n",
    "predictor.fit(X,y) # autogluon 에서 \"학습\"을 의미하는 과정\n",
    "model.fit(X,y) # sklearn 에서 \"학습\"을 의미하는 과정\n",
    "trainer.train() # huggingface 에서 \"학습\"을 의미하는 과정\n",
    "trainer.predict(dataset) # huggingface 에서 \"예측\"을 의미하는 과정\n",
    "model.fit(x, y, batch_size=32, epochs=10) # keras에서 \"학습\"을 의미하는 과정\n",
    "model.predict(test_img) # keras에서 \"예측\"을 의미하는 과정 \n",
    "```\n",
    "\n",
    "`-` R 코드..\n",
    "\n",
    "``` r\n",
    "# R\n",
    "ols <- lm(y~x) # 선형회귀분석에서 학습을 의미하는 함수\n",
    "ols$fitted.values # 선형회귀분석에서 yhat을 출력 \n",
    "predict(ols, newdata=test) # 선형회귀분석에서 test에 대한 예측값을 출력하는 함수\n",
    "ols$coef # 선형회귀분석에서 weight를 확인하는 방법\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **G. 신경망관련용어**\n",
    "\n",
    "-   ANN: 인공신경망\n",
    "-   MLP: 다층퍼셉트론 (레이어가 여러개 있어요)\n",
    "-   DNN: 깊은신경망, 심층신경망 - 얼마나 깊어야...? 대충 세 개 이상 정도...\n",
    "-   CNN: 합성곱신경망 - 컨볼루션 커널 쓰면 되니까 명확함\n",
    "-   RNN: 순환신경망 - 순환 구조가 있어야 함\n",
    "\n",
    "`# 예시1` – MLP, DNN\n",
    "\n",
    "``` python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),    \n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: O\n",
    "-   DNN: O\n",
    "-   CNN: X (합성곱레이어가 없으므로)\n",
    "-   RNN: X (순환구조가 없으므로)\n",
    "\n",
    "`#`\n",
    "\n",
    "`# 예시2` – MLP, Shallow Network\n",
    "\n",
    "``` python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=2,out_features=1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: O\n",
    "-   DNN: X (깊은 신경망으로 생각하려면 더 많은 레이어가 필요함. 합의된\n",
    "    기준은 히든레이어 2장이상, 이걸 설명하기 위해서 얕은 신경망이란\n",
    "    용어도 씀)\n",
    "-   CNN: X (합성곱레이어가 없으므로)\n",
    "-   RNN: X (순환구조가 없으므로)\n",
    "\n",
    "`#`\n",
    "\n",
    "`# 예시3` – MLP, DNN, Wide NN\n",
    "\n",
    "``` python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1048576),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1048576,out_features=1),\n",
    "    torch.nn.Sigmoid(),    \n",
    ")\n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: O\n",
    "-   DNN: O (깊긴한데 이정도면 모양이 깊다기 보다는 넓은 신경망임, 그래서\n",
    "    어떤 연구에서는 이걸 넓은 신경망이라 부르기도 함)\n",
    "-   CNN: X (합성곱레이어가 없으므로)\n",
    "-   RNN: X (순환구조가 없으므로)\n",
    "\n",
    "`# 예시4` – CNN\n",
    "\n",
    "``` python\n",
    "net = torch.nn.Sequential(\n",
    "    # Layer1\n",
    "    torch.nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    # Layer2\n",
    "    torch.nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    torch.nn.BatchNorm2d(128),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    # Layer3\n",
    "    torch.nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    torch.nn.BatchNorm2d(256),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    # Layer4\n",
    "    torch.nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "    torch.nn.BatchNorm2d(512),\n",
    "    torch.nn.LeakyReLU(0.2),\n",
    "    # Layer5\n",
    "    torch.nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: X (합성곱연결이 포함되어있으므로, MLP가 아님, 완전연결만\n",
    "    포함해야 MLP임)  \n",
    "-   DNN: O (그냥 레이어가 3층 이상이면 무지성으로 DNN임)\n",
    "-   CNN: O (합성곱레이어를 포함하고 있으므로)\n",
    "-   RNN: X (순환구조가 없으므로)\n",
    "\n",
    "`#`\n",
    "\n",
    "`# 예시5` – CNN\n",
    "\n",
    "``` python\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1,16,(5,5)),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d((2,2)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(2304,1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: X\n",
    "-   DNN: X? (히든레이어가 1장이므로..)\n",
    "-   CNN: O (합성곱레이어를 포함하고 있으므로)\n",
    "-   RNN: X (순환구조가 없으므로)\n",
    "\n",
    "> 근데 대부분의 문서에서는 CNN, RNN은 DNN의 한 종류로 설명하고\n",
    "> 있어서요.. 이런 네트워크에서는 개념충돌이 옵니다.\n",
    "\n",
    "`#`\n",
    "\n",
    "`# 예시6` – RNN\n",
    "\n",
    "``` python\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(4,2)\n",
    "        self.linr = torch.nn.Linear(2,2) \n",
    "    def forward(self,X):\n",
    "        h,_ = self.rnn(X) \n",
    "        netout = self.linr(h)\n",
    "        return netout \n",
    "net = Net()     \n",
    "```\n",
    "\n",
    "-   ANN: O\n",
    "-   MLP: X\n",
    "-   DNN: X? (히든레이어가 1장이므로..)\n",
    "-   CNN: X (합성곱레이어가 없으므로)\n",
    "-   RNN: O\n",
    "\n",
    "> 이것도 비슷한 개념충돌"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
