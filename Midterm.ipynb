{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **중간고사 이전 내용 총 정리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-Learning : 텐서**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. 벡터**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Vector 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Vector 덧셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, 2, 3]) + torch.tensor([2]*3)\n",
    "torch.tensor([1, 2, 3]) + 2 ## 브로드캐스팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **B. 벡터와 매트릭스**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 브로드 캐스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 브로드 캐스팅\n",
    "torch.tensor([[1, 2],\n",
    "              [3, 4],\n",
    "              [5, 6]]) - 1\n",
    "\n",
    "## 다른 연산\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]])*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 열별 브로드 캐스팅\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) + torch.tensor([[-1],\n",
    "                                                       [-3],\n",
    "                                                       [-5]])\n",
    "\n",
    "## 행별 브로드캐스팅\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) + torch.tensor([[-1, -2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 벡터와의 연산 : 열벡터로 취급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 4],\n",
       "        [4, 6],\n",
       "        [6, 8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) + torch.tensor([1, 2])\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) + torch.tensor([1, 2, 3]) ## 안됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 행렬곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정상적인 행렬곱\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) @ torch.tensor([[1],\n",
    "                                                       [2]])\n",
    "torch.tensor([[1, 2], [3, 4], [5, 6]]) @ torch.tensor([1, 2]) ## 열벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 벡터는 행렬곱 연산에서 열벡터도로, 행벡터로도 취급할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1, 2, 3]]) @ torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "torch.tensor([1, 2, 3]) @ torch.tensor([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **C. transpose, reshape**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 전치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2], [3, 4]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1) ## [[1, 2, 3], [4, 5, 6]]\n",
    "torch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n",
    "torch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **D. concat, stack**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` concat(차원 유지, 쉬움)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 5]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1], [3], [5]])\n",
    "b = torch.tensor([[2], [4], [5]])\n",
    "torch.concat([a, b], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` stack(차원 늘림, 어려움)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 3, 5])\n",
    "b = torch.tensor([2, 4, 6])\n",
    "torch.stack([a, b], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat([a.reshape(3, 1), b.reshape(3, 1)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 회귀**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. 내용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 단순선형회귀모형\n",
    "\n",
    "$$\\begin{align} y_i & = w_0 + w_1 x_i + \\epsilon, ~ i = 1, 2, \\cdots \\\\\n",
    "{\\bf y} & = {\\bf WX + \\epsilon}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 단순선형회귀에서 최적화하고자 하는 파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ loss(\\hat{w}_0,\\hat{w}_1) := loss(\\hat{\\bf W})=\\sum_{i=1}^{n}(y_i-(\\hat{w}_0+\\hat{w}_1x_i))^2=({\\bf y}-{\\bf X}{\\bf \\hat{W}})^\\top({\\bf y}-{\\bf X}{\\bf \\hat{W}})$$\n",
    "\n",
    "$$\\hat{\\bf W}^{LSE} = \\underset{\\bf \\hat{W}}{\\operatorname{argmin}} ~ loss(\\hat{\\bf W})$$\n",
    "\n",
    "> 위의 식은\n",
    "> $\\hat{\\bf W} = \\underset{\\bf W}{\\operatorname{argmin}} ~ loss({\\bf W})$\n",
    "> 로 생각해도 무방"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 쌩으로 구현(미분만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## data\n",
    "What = torch.tensor([[1.0], [2.0]], requires_grad = True) ## 매개변수 넣어야 미분 가능\n",
    "X = torch.stack([torch.ones(len(x)), x], axis = 1)\n",
    "\n",
    "##---##\n",
    "for epoc in range(100) :\n",
    "    yhat = X@What\n",
    "    \n",
    "    loss = torch.mean((yhat-y)**2)\n",
    "    loss.backward()\n",
    "\n",
    "    What.data -= 0.1*What.grad ## learning rate == 0.1\n",
    "    What.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 네트워크, 손실함수, 옵티마이저 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = torch.nn.Linear(in_features = 1, out_features = 1, bias = True) ## 이쪽이 일반적\n",
    "net = torch.nn.Linear(in_features = 2, out_features = 1, bias = False)\n",
    "net.weight.data = torch.tensor([[1.0, 2.0]]) ## 행벡터로 삽입\n",
    "# net.bias.data = torch.tensor([1.0])\n",
    "\n",
    "loss_fn = torch.nn.MSELoss() ## calla회귀 문제에서 일반적으로 사용\n",
    "optimizr = torch.optim.SGD(net.parameters(), lr = 0.1) ## iterabla obj : 데이터와 그래디언트\n",
    "\n",
    "##---##\n",
    "for epoc in range(300) :\n",
    "    yhat = net(X)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `bias`를 넣든, 안넣고 따로 설명변수 매트릭스를 넣든, 둘 다 하든지간에 `yhat`과 추정 결과는 동일함(다만 효율이랑 가시성이 좀 구리겠지...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. 로지스틱**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이진 분류 문제를 해결하기 위한 가장 기초적인 모형\n",
    ">\n",
    "> 순방향 구조, 선형 구조만 제대로 설명할 수 있다는 점에서 모형의 표현력이 다소 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   회귀모형: $y_i \\sim {\\cal N}(w_0+w_1x_i, \\sigma^2)$ -> 정규분포의 평균 예측\n",
    "\n",
    "-   로지스틱: $y_i \\sim {\\cal B}(\\pi_i),\\quad$ where $\\pi_i = \\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)} = \\frac{1}{1+\\exp(-w_0-w_1x_i)}$\n",
    "> 베르누이의 평균(확률값)을 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 쌩으로 구현(미분만 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "What = torch.tensor([[1.0],\n",
    "                     [1.0]], requires_grad = True)\n",
    "\n",
    "def sigmoid(x) :\n",
    "    return 1/(1+torch.exp(-(What.data[0] + What.data[1]*x)))\n",
    "\n",
    "for epoc in range(300) :\n",
    "    yhat = sidmoid(x)\n",
    "\n",
    "    ## MLE니까 가능도가 가장 높은 -> -l이 가장 작은 것을 찾는 최적화 문제로 바꿈\n",
    "    loss = -torch.sum(y*torch.log(yhat) + (1-y)*torch.log(1-yhat)) ## 강의 노트엔 그냥 MSELoss 쓰긴 함...\n",
    "    loss.backward()\n",
    "    \n",
    "    What.data -= 0.1*What.grad\n",
    "    What.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 네트워크, BCELoss, Adam 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 1), ## 이진 분류문제이므로 y의 차원은 1\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "net[0].weight.data = torch.tensor([[1.0, 1.0]])\n",
    "net[0].bias.data = torch.tensor([1.0])\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "## 지역 최소값에 잘 빠지지 않고, 최적화 속도가 빠름\n",
    "optimizr = torch.optim.Adam(net.parameters(), lr = 0.1)\n",
    "\n",
    "##---##\n",
    "for epoc in range(300) :\n",
    "    yhat = net(X)\n",
    "    \n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. 신경망, ReLU, 사용자 정의 네트워크**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 시그모이드에 넣기 전, 꺾인 그래프를 만들어 로지스틱의 표현력을 확보\n",
    "* 신경망의 경우 회귀분석과 달리 여러 개의 최적값이 존재할 수 있음 -> 한 개의 global minimum만 가지지 않을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-10, 10).float()\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "## v자 그래프\n",
    "relu(x) + relu(-x)\n",
    "\n",
    "## A자 그래프\n",
    "- relu(x) - relu(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 브로드캐스팅\n",
    "u = torch.stack([x, -x], axis = 1)\n",
    "v = relu(u)\n",
    "\n",
    "-4.5*v[:, [0]] - 9.0*v[:, [1]] + 4.5 ## 이 경우 [0]번 열이 뒤쪽, [1]번 열이 앞쪽 모양을 담당\n",
    "\n",
    "## 선형 결합\n",
    "l2 = torch.nn.Linear(2, 1, bias = True)\n",
    "l2.weight.data = torch.tensor([[-4.5, -9.0]])\n",
    "l2.bias.data = torch.tensor([4.5])\n",
    "\n",
    "l2(v)\n",
    "\n",
    "## 두 번의 선형 결합\n",
    "l1 = torch.nn.Linear(1, 2)\n",
    "l1.weight.data = torch.tensor([[1.0], [-1.0]]) ## 실제로 행벡터를 넣어야 하니 반대로...\n",
    "l1.bias.data = l1.bias.data*0\n",
    "\n",
    "l2(relu(l1(x.reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기초적인 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "##---##\n",
    "for epoc in range(300) :\n",
    "    yhat = net(X)\n",
    "\n",
    "    loss = loss_fn(yhat, y)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` `H`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H(torch.nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super.__init__() ## 슈퍼 클래스의 __init__을 그대로 상속\n",
    "\n",
    "    def forward(self, u) :\n",
    "        h = lambda x : torch.sigmoid(200*(x+0.5)) + torch.sigmoid(-200*(x-0.5)) - 1.0\n",
    "        v = h(u)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. 시벤코 정리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 하나의 은닉층을 가지는 네트워크는 모든 보렐 가측함수 $f: {\\bf X}_{n \\times p} \\to {\\bf y}_{n\\times q}$를 원하는 정확도로 근사시킬 수 있음\n",
    ">\n",
    "> 이 때, 은닉층의 활성화함수는 어떤 것이여도 상관없음 (`ReLU`, `Sidmoid`, `H`, ...)\n",
    ">\n",
    "> 즉, 하나의 은닉층을 가진 신경망의 표현력은 거의 무한대라 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분류 모형의 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1024),\n",
    "    H(), ## ReLU, Sigmoid...\n",
    "    torch.nn.Linear(1024, 1),\n",
    "    torch.nn.Sigmoid() ## 시그모이드를 넣어줌\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 회귀 모형의 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1024),\n",
    "    H(), ## ReLU, Sigmoid...\n",
    "    torch.nn.Linear(1024, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
