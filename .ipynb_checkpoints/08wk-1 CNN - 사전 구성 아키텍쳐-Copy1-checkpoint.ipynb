{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bac18bf",
   "metadata": {},
   "source": [
    "# 08wk-1: (합성곱신경망) – MNIST, CIFAR10, XAI란?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff267511-cf16-4c2d-99ad-c7fdada0cf3f",
   "metadata": {},
   "source": [
    "다음주 대면수업 없음 ㅇㅇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20c324",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6caaef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611fd427-c549-468d-838c-8329d96a9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4.5, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e86b8",
   "metadata": {},
   "source": [
    "## 2. 주요 코드 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d6e44-ed26-44e3-bfa4-0c1128f0c3e9",
   "metadata": {},
   "source": [
    "`-` 알렉스넷 아키텍쳐\n",
    "\n",
    "* Conv : kernel_size = 11, stride = 4\n",
    "> 먼저 러프하게 보고 싶음 : 처음엔 큰 이미지를 대충대충 보고 싶음\n",
    "\n",
    "* Pool : kernel_size = 3, stride = 2\n",
    "> 처음엔 요약하고, 나중엔 요약하지 않고 싶음\n",
    "\n",
    "* Conv : kernel_size = 3\n",
    "> 조금 세밀하게 보고 싶음\n",
    "\n",
    "* 마지막 Pool\n",
    "> Flatten하기 직전이고, 데이터가 좀 많은 것 같아서 줄여줌\n",
    "\n",
    "* 신경망 설계 부분\n",
    "> ReLU : 1d part에서 표현력을 좀 더 얻어내고 싶다.\n",
    ">\n",
    "> 요약하고 싶었으면 Linear 한층만 받아도 충분했겠죠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45259f23-1874-4c3a-be97-c1b715273c42",
   "metadata": {},
   "source": [
    "`-` Pytorch\n",
    "\n",
    "```Python\n",
    "## 파이토치는 y가 정수형이면 알아서 범주형으로 원-핫 인코딩을 수행함\n",
    "torch.cuda.empty_cache() ## GPU 메모리 초기화\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b76f9a-0cd6-4bea-aa1e-bc1d4719ebde",
   "metadata": {},
   "source": [
    "`-` 사전구성 아키텍쳐\n",
    "\n",
    "```Python\n",
    "net = torchvision.models.resnet18(pretrained = True) ## 학습된 가중치까지 같이 가져옴\n",
    "net.fc = torch.nn.Linear(512, 10) ## 레이블의 개수. not iterable. 딕셔너리나 텐서처럼 불러와야 하는듯\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329cd8d3-2717-4812-8a67-d23a016f2517",
   "metadata": {},
   "source": [
    "## 3. MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d601f91-0a6a-47b3-887a-26ac6685542a",
   "metadata": {},
   "source": [
    "`#-#` 지난 시간 복습...\n",
    "\n",
    "* CNN\n",
    "\n",
    "> `net = 2d -> 1d`\n",
    ">\n",
    "> (linr -> ReLU) : 선형변환 -> 비선형변환\n",
    ">\n",
    "> (conv -> ReLU -> MP) : 선형변환(행렬곱이니까...) -> 비선형변환 -> 비선형변환\n",
    ">\n",
    "> > conv는 특징을 더 다양하게 하기 위한 선형 변환 - conv만 더 많이 취한다고 해서 표현력이 좋아지는 게 아님\n",
    "> >\n",
    "> > ReLU는 특징을 더 다양하게 하기 위한 비선형 변환\n",
    "> >\n",
    "> > MP는 특징을 더 단순하게 키우기 위한 비선형 변환 -> 요약을 하고 싶은 건데 스트라이딩해서 누를 필요 없음\n",
    "> \n",
    "> 아키텍쳐만 보고도 왜 이렇게 레이어를 잡았는지 파악할 수 있어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa4efd-b578-4327-9ab8-e4b6080b57d6",
   "metadata": {},
   "source": [
    "`-` 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "244f0167-4d0b-473a-a12c-cc50da857f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True,transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True,transform=torchvision.transforms.ToTensor())\n",
    "X,y = next(iter(torch.utils.data.DataLoader(train_dataset,batch_size=6000,shuffle=True)))\n",
    "XX,yy = next(iter(torch.utils.data.DataLoader(train_dataset,batch_size=1000,shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9008b3d6-1ad5-4da1-bbbe-6e200fe61fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6000, 1, 28, 28]), torch.Size([6000]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape ## 6000장의 흑백 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0871bb59-5924-4043-96df-35a420e01e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1, 28, 28]), torch.Size([1000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.shape, yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bacbc65-3bf5-4442-8174-f6d6370fde35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 6, 6,  ..., 9, 6, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y ## one-hot encoding이 필요함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcab20-6233-434e-86e3-154bed894e87",
   "metadata": {},
   "source": [
    "> one-hot encoding해야 하네? -> 아님\n",
    ">\n",
    "> 파이토치는 정수형 자료를 자동으로 범주형으로 인식, 알아서 원-핫 인코딩 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c6a907-add2-4827-945c-939e71e3f5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 3200])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dffb74f7-7a11-455c-9fb6-6fa74509fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5), ## 선형 - (n, 32, 24, 24)\n",
    "    torch.nn.ReLU(), ## 비선형\n",
    "    torch.nn.MaxPool2d(kernel_size = 2), ## 비선형 - (n, 32, ???, ???)\n",
    "    ##---레이어 추가---##\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3), ## 위보다 부분적으로 집중해서 보겠다\n",
    "    torch.nn.ReLU(),\n",
    "    # torch.nn.MaxPool2d(kernel_size = 2), ## 이미 위에서 충분히 요약된 것 같은데, 또 줄이긴 싫다\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(3200, 10)\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "#---#\n",
    "# ds = torch.utils.data.TensorDataset(X)\n",
    "# dl = torch.utils.data.DataLoader(ds, batch_size = 1024)\n",
    "\n",
    "X.to(\"cuda:0\")\n",
    "y.to(\"cuda:0\")\n",
    "\n",
    "for epoc in range(100) :\n",
    "    netout = net(X) ## yhat 자체 아님, 로짓임\n",
    "    loss = loss_fn(netout, y)\n",
    "    loss.backward()\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6b6279e-dc74-4275-834b-56b58af872cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9742, device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "808e662f-bd20-4c2e-ba2f-c41d99b50813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9740, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9dc7f0-d2d5-4bdc-8087-198360cbf0ad",
   "metadata": {},
   "source": [
    "> 개잘함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c70d521e-6d05-496b-9eef-939957be6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() ## 캐시 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff852613-3640-424c-bea8-4af8bd040dba",
   "metadata": {},
   "source": [
    "## 4. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3cbff1d-fd82-4749-920a-84c99dff82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,transform=torchvision.transforms.ToTensor())\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,transform=torchvision.transforms.ToTensor())\n",
    "X,y = next(iter(torch.utils.data.DataLoader(train_dataset,batch_size=10000,shuffle=True)))\n",
    "XX,yy = next(iter(torch.utils.data.DataLoader(train_dataset,batch_size=2000,shuffle=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a19381-00bf-4400-9d27-8d94bbffcae0",
   "metadata": {},
   "source": [
    "### A. 직접 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f06bf48-15e5-4444-88e0-522df25e41fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4b2ac69bb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.einsum(\"chw -> hwc\", X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b24a1a-9ada-48ea-91be-a33d9e172e94",
   "metadata": {},
   "source": [
    "> 컬러 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b343ac3-d8d7-4f93-a150-3ed93dfe2891",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 5), ## 선형 - (n, 32, 24, 24)\n",
    "    torch.nn.ReLU(), ## 비선형\n",
    "    torch.nn.MaxPool2d(kernel_size = 2), ## 비선형 - (n, 32, ???, ???)\n",
    "    ##---레이어 추가---##\n",
    "    torch.nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3), ## 위보다 부분적으로 집중해서 보겠다\n",
    "    torch.nn.ReLU(),\n",
    "    # torch.nn.MaxPool2d(kernel_size = 2), ## 이미 위에서 충분히 요약된 것 같은데, 또 줄이긴 싫다\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(4608, 10)\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "\n",
    "#---#\n",
    "# ds = torch.utils.data.TensorDataset(X)\n",
    "# dl = torch.utils.data.DataLoader(ds, batch_size = 1024)\n",
    "\n",
    "X = X.to(\"cuda:0\")\n",
    "y = y.to(\"cuda:0\")\n",
    "XX = XX.to(\"cuda:0\")\n",
    "yy = yy.to(\"cuda:0\")\n",
    "\n",
    "for epoc in range(100) :\n",
    "    netout = net(X) ## yhat 자체 아님, 로짓임\n",
    "    loss = loss_fn(netout, y)\n",
    "    loss.backward()\n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af2f7550-5588-4cd9-97a4-2f86e09ab02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5130, device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92789e74-738c-4381-8262-84aee93b1874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4765, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e1ecc-09c3-4e81-8250-2e00496ce752",
   "metadata": {},
   "source": [
    "> 표현력이 좀 모자란 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aaaa01a7-5ff0-4de1-bfc1-73bcc090347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b572e-cd69-4403-b6c1-224be891c9a9",
   "metadata": {},
   "source": [
    "### B. 알렉스넷?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Comparison_image_neural_networks.svg/960px-Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095894f3-72aa-42da-b33a-87c753bf6eec",
   "metadata": {},
   "source": [
    "* Conv : kernel_size = 11, stride = 4\n",
    "> 먼저 러프하게 보고 싶음 : 처음엔 큰 이미지를 대충대충 보고 싶음\n",
    "\n",
    "* Pool : kernel_size = 3, stride = 2\n",
    "> 처음엔 요약하고, 나중엔 요약하지 않고 싶음\n",
    "\n",
    "* Conv : kernel_size = 3\n",
    "> 조금 세밀하게 보고 싶음\n",
    "\n",
    "* 마지막 Pool\n",
    "> Flatten하기 직전이고, 데이터가 좀 많은 것 같아서 줄여줌\n",
    "\n",
    "* 신경망 설계 부분\n",
    "> ReLU : 1d part에서 표현력을 좀 더 얻어내고 싶다.\n",
    ">\n",
    "> 요약하고 싶었으면 Linear 한층만 받아도 충분했겠죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ccb5ac7a-7d81-4d69-b2d0-d118a56c5e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.randn(3, 224, 224).reshape(1, 3, 224, 224)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d42512a-b816-4d98-9d4c-f4474e843470",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 96, 11, 4), ## 54x54 이미지, 96개 채널(96 유형의 이?미지)\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size = 3, stride = 2), ## 26x26 이미지\n",
    "    torch.nn.Conv2d(96, 256, 5, padding = 2), ## 외곽 공간에 값이 있다고 치면서 컨볼루션 -> 이미지 크기 고정을 위함\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size = 3, stride = 2), ## 12x12 이미지\n",
    "    torch.nn.Conv2d(256, 384, kernel_size = 3, padding = 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384, 384, kernel_size = 3, padding = 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(384, 256, kernel_size = 3, padding = 1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size = 3, stride = 2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(6400, 4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.dropout(0.5),\n",
    "    torch.nn.Linear(4096, 4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.dropout(0.5),\n",
    "    torch.nn.Linear(4096, 1000) ## output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dca4bc99-5624-40fe-97a7-3134f839d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d261642-94e8-4049-8bd7-01a300bb9893",
   "metadata": {},
   "source": [
    "### C. 알렉스넷으로 ImageNet 적합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "858fba61-5d09-4b0e-b571-202e236f11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[-1] = torch.nn.Linear(4096, 10) ## 1000 클래스가 아니라 10 클래스임..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a08cd588-ef19-44ca-926c-9daba0305209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = torch.randn(1,3,32,32)\n",
    "# net(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c09f792-382c-4bf6-9b8e-4eacd0f00fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 2, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[:5](img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f9a4754-38ba-4e53-8f72-61cdb1c89108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=(3, 3), stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f33cc09-6255-415d-ba4f-744889567000",
   "metadata": {},
   "source": [
    "안됨 ㅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d6456-e1bb-48d0-b1bf-da2c66d59c14",
   "metadata": {},
   "source": [
    "> 채널은 256, 이미지 크기는 2x2인데 MP 윈도우 사이즈는 3이여서 찡김...\n",
    ">\n",
    "> 이렇게 이미지 크기가 좀 차이만 나도 네트워크 다시 짜야되나...???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7f575-3ead-4ae3-b0ba-c7f82100f712",
   "metadata": {},
   "source": [
    "### D. renset18\n",
    "\n",
    "`-` res: <https://arxiv.org/pdf/1512.03385>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4334ce58-289a-4a11-9991-9c066930f2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torchvision.models.resnet18() ## 이미 설계된 아키텍쳐를 가져옴\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165db45-3fa2-444f-b5f1-0cff43ae89bd",
   "metadata": {},
   "source": [
    "> 마지막 `Linear()`부분만 좀 바꾸면 좋을 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9091b9d0-434b-4889-ae40-7a6ad9a3a530",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ResNet' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ResNet' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "net[-1] ## sequential이 아님. not subscriptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d31c69c-4133-453e-b407-359c96cb433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc = torch.nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03c5eb2e-3b79-42ef-bae8-1e6191d12cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "#---#\n",
    "net.to(\"cuda:0\")\n",
    "X = X.to(\"cuda:0\")\n",
    "y = y.to(\"cuda:0\")\n",
    "XX = XX.to(\"cuda:0\")\n",
    "yy = yy.to(\"cuda:0\")\n",
    "#---#\n",
    "for epoc in range(500):\n",
    "    #1\n",
    "    netout = net(X)\n",
    "    #2\n",
    "    loss = loss_fn(netout,y)\n",
    "    #3\n",
    "    loss.backward()\n",
    "    #4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4034f70-fd38-4fef-927a-12fcd69540f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa702e60-9051-468d-8cec-0cd7c2fa95dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b75f2150-dd81-4a73-b545-e79ec4e8a9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5940, device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a5b79-c243-4205-9d75-5721a3dd54ec",
   "metadata": {},
   "source": [
    "> 일단 표현력은 충분히 잘 나옴. 이제 오버피팅만 해결하면 되겠구나!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24edee81-cb9e-4db7-9245-0da87ce0c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540df54e-a418-4dd8-9d03-6a9deb346984",
   "metadata": {},
   "source": [
    "### E. resnet18, pretrained = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dec0cc-8cf0-4314-bc03-0e5be22a613c",
   "metadata": {},
   "source": [
    "> A를 잘하는 네트워크는 B도 잘할 것 같지 않아?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8cf8777a-94f0-4f39-998d-c44a27d26b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet18(pretrained = True) ## 학습된 가중치까지 같이 가져옴\n",
    "net.fc = torch.nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "764cb68d-d0da-4340-88d1-b357c79e783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizr = torch.optim.Adam(net.parameters())\n",
    "\n",
    "#---#\n",
    "net.to(\"cuda:0\")\n",
    "X = X.to(\"cuda:0\")\n",
    "y = y.to(\"cuda:0\")\n",
    "XX = XX.to(\"cuda:0\")\n",
    "yy = yy.to(\"cuda:0\")\n",
    "#---#\n",
    "for epoc in range(500):\n",
    "    #1\n",
    "    netout = net(X)\n",
    "    #2\n",
    "    loss = loss_fn(netout,y)\n",
    "    #3\n",
    "    loss.backward()\n",
    "    #4 \n",
    "    optimizr.step()\n",
    "    optimizr.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb59f3-c766-40f4-90c5-5d2c63456d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a47b5c65-7f74-453a-ba66-452e0b81b6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(X).argmax(axis=1) == y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1c8cb6b-6bb6-48f4-8b6f-36e9156421bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000, device='cuda:0')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net(XX).argmax(axis=1) == yy).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4ed81-1c83-4fc3-91a0-9176f8d4a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff57d7-f170-4fb2-9c09-52032cde34a6",
   "metadata": {},
   "source": [
    "## 5. XAI (eXplainable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6750e2-ffa9-4d09-ad92-ace2fad32f41",
   "metadata": {},
   "source": [
    "<https://brunch.co.kr/@hvnpoet/140>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
